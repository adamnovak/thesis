%% uctest.tex 11/3/94
%% Copyright (C) 1988-2004 Daniel Gildea, BBF, Ethan Munson.
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2003/12/01 or later.
%
% This work has the LPPL maintenance status "maintained".
% 
% The Current Maintainer of this work is Daniel Gildea.

\documentclass[11pt,proposal]{ucthesis}
\def\dsp{\def\baselinestretch{2.0}\large\normalsize}
\dsp

% 2010june01 sol katzman:
% package geometry should override the various margin settings from .clo and .cls
% and also eliminates issues where the default papersize is A4
\usepackage[letterpaper, left=1.5in, right=1.25in, top=1.25in, bottom=1.25in, includefoot]{geometry}

\usepackage{url}

\begin{document}

% Declarations for Front Matter

\title{Infrastructure for Scalable Analysis of Genomic Variation}
\author{Adam M. Novak}
\degreeyear{2014}
\degreemonth{June}
\degree{DOCTOR OF PHILOSOPHY}
\chair{Professor David Haussler}
\committeememberone{Professor Ivory Insular}
\committeemembertwo{Professor General Reference}
\committeememberthree{Professor Ipsum Lorem}
\numberofmembers{4} %% (including chair) possible: 3, 4, 5, 6
\deanlineone{Dean John Doe}
\deanlinetwo{Vice Provost and Dean of Graduate Studies}
\deanlinethree{}
\field{Bioinformatics}
\campus{Santa Cruz}

\begin{frontmatter}

\maketitle
\copyrightpage

\tableofcontents
\listoffigures
\listoftables

\begin{abstract}

\end{abstract}

\begin{acknowledgements}

\end{acknowledgements}

\end{frontmatter}

\chapter{Introduction}

Biology happens at scale. Scale in population and in time give evolution the raw materials it needs to shape biological systems worth studying. Scale enables bacteria to become resistant to antibiotics. Scale allows cancer to arise with alarming regularity in healthy people. Scale powers the adaptive immune system, and simultaneously enables pathogens to evade it. To really understand biological systems, we need to be able to match their scale.

In recent years, in part due to a precipitous drop in the cost of DNA sequencing, the scale of biological data collection has dramatically increased \cite{wetterstrand2014dna}. Unfortunately, it has increased disproportionately along the ``number of features'' axis, as opposed to the ``number of samples'' axis. The Cancer Genome Atlas, for example, has collected many types of data, including genomic DNA sequences, DNA methylation data, and mRNA expression levels, amounting to millions of features per sample \cite{tcga2014sample}. However, it has looked at a number of individual people and cancers only on the order of thousands \cite{tcga2014sample}. Similarly, while the 1000 Genomes Project found millions of unique genomic variants, it examined only on the order of a thousand individuals \cite{10002010map}. While such sample sizes may sound large to those who remember the years of effort and multi-billion-dollar expenditure required by the original Human Genome Project, they are woefully small compared to the numbers of features per sample. From a machine learning perspective, this limits the amount of useful knowledge that can be extracted from all of that data. If a theory is to be expected to generalize to new data (that is, if it is to actually reflect the biological processes at work), it generally ought to be based on more data points than features \cite{hua2005optimal}.

For now, it is still possible to use such small sample sizes to do meaningful biology \cite{weinstein2013cancer}. However, at some point in the future, we will exhaust what we can learn by combining our prior knowledge of genetics and biochemistry with a few thousand high-throughput sequencing samples, and we will need to scale up data collection and data analysis in terms of number of samples.

Unfortunately, to scale by even a few orders of magnitude in sample sizes, the way bioinformatics is done will need to change. As it is, working on huge datasets like the sequencing reads from TCGA can be an enormous challenge at the merely technical level, with thousands of gigabytes of data to transfer, store, and protect from unauthorized access. Even lifting over such a huge data set to the new build of the reference genome is going to be a formidable task.

At some point, as we increase the number of samples, we will have to abandon the idea that every lab that wants to analyze these enormous community data sets ought to have its own local copy.

Moreover, when we start to scale up to large numbers of samples, our data sets are more likely to contain individuals who are in some way out of the ordinary, and consequently our engineering standards need to be tightened. For example, if a bioinformatics analysis pipeline assumes that a certain run of genes occur in a certain order and orientation, an increased number of samples pushed through this pipeline translates into an increased chance of encountering an individual who contradicts that assumption, resulting in at best a run-time error and an unhappy bioinformaticist, and at worst a subtly incorrect result in a published paper. All the corner cases---the pathological combinations of variants that we just assume won't occur, or the genomic regions that we would rather not talk about---need to be found and addressed if bioinformatic analyses are going to be made to scale to the sizes they need to scale to.

This means that in order to scale up bioinformatics, we will need to solve a software quality problem. It takes more robust software to handle a million samples than to handle a thousand samples. One way to try to solve this problem might be to make bioinformaticists into better software engineers. A better approach might be to provide them with more robust and more complete software libraries, which provide abstractions that can safely be applied to large numbers of samples, and which consistently expose the now-relatively-frequent edge cases which bioinformaticists will need to deal with at these scales. This latter approach is the one that I intend to take.

One of the abstractions which unfortunately cannot be safely applied at these scales is the idea, central to current bioinformatics practice, of a universal human reference genome and its associated linear coordinate space. When you have only one sequenced genome, it's perfectly reasonable to do things like extract a 2 kilobase window of sequence centered on an arbitrary position. However, we now have a reasonable number of sequenced genomes---enough to get a sense of what common variation exists in the human population, although not enough to understand the significance of many of these \cite{10002010map}---and it is growing increasingly clear that that sort of operation only makes sense in certain cases. In the latest release of the human reference genome, hg38, there are 261 ``alt loci'', or pieces of sequence which are not on the main reference chromosomes, but which model arrangements of genes and other genomic elements which are present in a non-negligible fraction of people \cite{karolchik2104new}. In genomic regions where these alternate haplotypes apply, the traditional linear coordinate system, which refers to locations in peoples' genomes by the chromosome and base index in the reference genome begins to break down. To properly reason about such genomic regions, we need to abandon either the idea that bases in peoples' genomes correspond to bases in a reference, or the idea that the bases in a reference come in a reasonably-defined linear order under a useful linear coordinate system.

% TODO: Note about alt loci messing up mapping. Can I demo this or something?

The linear organization of the reference genome also frustrates attempts to study regions of the genome which are difficult to assemble, or which, due to sequence similarity, are very difficult to distinguish from similar regions at other locations in the genome. To facilitate analysis of the centromeres, for example, hg38 includes imaginary, plausible linear centromere sequences \cite{karolchik2104new}. We have more precise, graph-based models of what we actually know about the centromeres, but these models cannot be indexed by linear sequence coordinates or processed by tools that expect a linear reference sequence \cite{miga2014centromere}.

I propose a nonlinear, graph-based reference for human genomes. Such a representation can capture in a first-class way the sequence information which is currently relegated to alternate haplotypes, as well as additional variant information from other sources. Combined with a rigorous definition of mapping, such a scheme can potentially combat allele-specific mapping bias \cite{degner2009effect}. Furthermore, such a representation allows for ``collapsing'' ambiguous regions together, with variable stringency, permitting the inclusion of a more faithful representation of our knowledge of centromeres and other repetitive or ambiguous regions of the genome.

For my thesis project, I intend to formalize the mathematics of constructing and mapping to such a reference, build scalable software tools and API infrastructure for creating and working with such a reference, and use a constructed reference of this form to reach new, biologically-relevant conclusions.

% Biology only works at scale

    % Scale in population and in time is what allows evolution to work
    
    % Scale produces drug resistance in bacteria
    
    % Scale produces cancer in healthy people
    
    % We won't be able to understand biology until we can match its scale

% High throughput is along the wrong dimension (so far)
    
    % We have a lot of features of a few samples.
    
    % To some extent we can get by with clever algorithms and educated guesses as prior knowledge.

% We need to be able to handle millions or even billions of samples if we want to be able to do biology just by looking.

    % Cancer, for example, won't make sense until we do this
    
% We need to be able to do this with something not millions of times better than current computing hardware

     % Because the transistors are now nearly as small as the stuff we're sequencing

% We can't scale that big by pushing BAM files around.

    % A billion 100-gig BAM files is 93 exabytes. That is too big to download.

% 1 in a million things start to happen, and we need infrastructure to handle them
    
    % We need to handle overlapping variants
    
    % We can't just throw out the alternate haplotypes
    
    % Assuming that everyone is contiguous in reference genome coordinates is going to get us into trouble
    
    % We can't keep messing around with the reference coordinates because we can't keep re-mapping every single read ever read.
    
    % We'll want to look at important variation in repetitive areas, and we may not yet have long reads

% So I'm going to build some stuff we will need in order to make this work
    
    % A non-linear coordinate system with stable base identifiers
    
    % A reference graph
    
        % Can capture more than just a single reference sequence
        
        % Reduce mapping bias
        
        % Probably less racist
        
    % A deterministic way to identify observed bases within that graph
        
    % A hierarchy of different versions of this graph with differing levels of specificity
        
        % Interlinked so you can project up and down for a rigorous notion of multimaping
        
% Then I'm going to try it out and discover something useful.
        
\chapter{Background}

% How bioinformatics in general works

    % Download data
    
    % Analyze data
    
    % Already a huge pain with TCGA
    
        % Anyone who wants to work with the data needs a place to locally mirror it
        
        % And that place has to meet TCGA's exacting standards for data security and access control.
        
        % It would be much easier if TCGA could just do all the access control itself.
        
    % This is not going to be able to scale much bigger. Already it's very difficult for smaller labs to use these huge community data sets.

% How genomics works

     % We have The Human Reference Genome
     
     % This gives us a linear contig-base coordinate space
     
     % We map reads to and define genes on that coordinate space
     
        % And define "variation" as any deviation from that sequence
        
        % We have mapping bias which is probably racist
     
     % Everybody uses it, and when we update it everybody has to upgrade
     
        % All the coordinates change
        
        % Published coordinates from back in the day no longer work and need to be lifted over or even remapped

% HG38 changes

    % Version jump
    
    % Lots of alt haplotypes
    
    % Plausible centromeres
    
% How variant analysis works

    % Make big VCFs
    
    % Store all the records as version of a variant at a coordinate position.
    
    % Ignore the fact that people have alternate haplotypes
    
        % No way to specify or look up which haplotype a person has
        
        % Only some coordinate positions are defined for any given person, so some variants are nonsensical for some people.
        
    % The same variant can be called different ways depending on the aligner
    
    % And the same variant can be written in any of three different syntaxes

        % Each of which results in completely different parser data structures available to analysis code
        
    % VCF creates complexity where it isn't and turns holes in the abstractions into pitfall traps for unwary bioinformaticists.
    
% So the old way of doing things is fraying around the edges

% We think we can make a better way using a few key technologies

    % We want to make a graph reference
        
        % We already have haplotypes bubbling off, so we really have a graph already
        
        % Benedict did all this previous work on graph genomes
        
    % We want to put this in an aligned hierarchy
        
        % Like something out of HAL, but with a different interpretation.
    
    % We want to employ FMD-index technology and succinct data structures to map to it uniquely at any given level
    
    % We want to employ Spark/GraphX to build a system to traverse and query these graphs
    
    % We want to employ Avro to give this system a consistent API accessible from a variety of languages.
    
% When this is done, I anticipate it being useful for things like NOTCH2NL
    
    % NOTCH2NL used to be annotated as one gene in hg19
    
    % In hg38 we've worked out that it is really 4 similar genes, -A through -D.
    
    % Now we have 4 very similar genes, near each other, and we want to look at variation in them.
    
    % We have all this microarray data and sequencing data that's super hard to pin down to one and only one paralog.
    
    % To get something out of this region, we need a way to work with multimapped data in a rigorous way.

\chapter{Preliminary Work}

% I have already some some work on infrastructure for the scalable analysis of genomic variation.

\section{Project \#1: Tumor Map Visualization}

% I made a pan-cancer visualization of the entire cross-tissue TCGA data set
    
    % Lay out all the tumors by a similarity metric, and squish them into a hexagonal grid in 2D
    
    % Let people fly around it with Google Maps
    
    % Then draw map overlays depicting various tumor or patient attributes.

    % Less a scaling up of the analysis than a scaling down of the data set into a space where people are comfortable working.
    
% See the paper. 

\section{Project \#2: NOTCH2NL Copy Number Variation Analysis}

% I made a first pass at analyzing the NOTCH2NL region
    
    % I specifically worked on a linear programming based approach to make copy number calls on our corrected assembly using CGH microarray probes originally designed for an assembly that conflated the NOTCH2NL genes
    
    % Trying to handle multimapping, because if I threw out multimapping I would have thrown out XX% of the data.
        
        % This again isn't really scaling, but it's something we need to do to support scaling.
    
% We found that CNV in the NOTCH2NL region can explain disease.

    % See the paper.

\section{Project \#3: Mapping to a Reference Genome Structure}

% We developed a rigorous theory for making reference graphs, putting them in hierarchies, and mapping to them based on context.

% I'm going to need this in my system as an alternative to linear coordinates, since linear coordinates have all the problems mentioned above.

% The hierarchy stuff is also how I plan to formalize multimapping into something useful.

% See the paper.

\chapter{Proposed Work}

\section{Specific Aim \#1: Develop Reference Hierarchy Theory}

% I need a rigorous theory of reference structures, reference hierarchies, mapping, and multimapping

% Also a mathematics of variants as paths, and a reference as a collection of equal variants.

% This is mostly completed.

\section{Specific Aim \#2: Engineer Scalable Software Tools}

% I need to turn Specific Aim #1 from math into software.

% To be minimally useful it needs to be able to do at least what we do now: map to hg38 plus alt haplotypes
    
    % But in a graph-based way
    
% What I actually want to do is to be able to map to hg38 and alts, plus a bunch of other variants.

    % I'd like to show that this reduces reference bias in mapping
    
% I don't plan to scale to more than tens of genomes' worth of sequence in the reference within the scope of this project.

    % I can fit a bunch of examples of variants in without full genomes to hold them.
    
% I'm going to stick an API on it so the software is accessible from multiple languages

    % Ideally with RPC so you don't need to mirror this whole graph structure in order to play with it.

\section{Specific Aim \#3: Discover Biologically Relevant Variation}

% Once I've built the system, I'll browse around the graph and see if I can find anything interesting.

% Since this system will be well-designed for analyzing structural variants, I will see if I can compile some broad statistics about the number and type of structural variants in my reference versus in other compendiums of structural variation.

% If I can't find anything else, I could take another look at the NOTCH2NL region, and repeat my analysis of the CGH microarray data in graph terms.

\appendix
\chapter{Some Ancillary Stuff}

Ancillary material should be put in appendices, which appear BEFORE the
bibliography. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bibliography

% 2010june01 sol katzman:
% if \nocite is specified, all entries in the bib file are included,
% probably not what you want, so comment out the \nocite and only get the cited references.
\nocite{*}

% 2010june01 sol katzman:
% this makes the bibliography single spaced, with double spacing between entries
\def\baselinestretch{1.0}\large\normalsize

\bibliographystyle{plain}
\bibliography{proposal}

\end{document}
